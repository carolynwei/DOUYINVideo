# -*- coding: utf-8 -*-
"""
è§†é¢‘å¼•æ“æ¨¡å—ï¼šå¤„ç†è§†é¢‘æ¸²æŸ“ã€TTSåˆæˆã€BGMæ··éŸ³ç­‰åŠŸèƒ½
VideoTaxi ç‰‡æ®µå¼æƒ…ç»ªå¼•æ“ (Segmented Emotional Engine)
ç¡®ä¿æ‰€æœ‰ä¸­æ–‡å­—ç¬¦æ­£ç¡®æ˜¾ç¤¾
"""

import os
import platform
import asyncio
import edge_tts
import numpy as np
import requests
import json
import base64
import uuid
import subprocess
import sys
import random
from PIL import Image, ImageDraw, ImageFont
import streamlit as st
from moviepy.editor import AudioFileClip, ImageClip, ColorClip, CompositeVideoClip, concatenate_videoclips, CompositeAudioClip, afx, concatenate_audioclips

# ğŸ­ æƒ…ç»ª-å‚æ•°è·¯ç”±è¡¨ (Emotion-Parameter Routing Table)
# åŸºäºâ€œè¯­ä¹‰-æƒ…ç»ªæ˜ å°„â€çš„å·¥ä¸šåŒ–æ¶æ„
VIBE_ROUTING_TABLE = {
    # å†·é™ç±»
    "cold_question": {
        "desc": "æ²‰ç¨³/å†·æ·¡/è´¨ç–‘",
        "edge_params": {"rate": "-5%", "pitch": "0%", "volume": "+0%"},
        "volc_voice": "zh_male_junlangnanyou_emo_v2_mars_bigtts",  # ä¿Šæœ—ç”·å‹-å†·é™
    },
    "deep_mystery": {
        "desc": "æ‚¬ç–‘/ä½æ²‰/ç¥ç§˜",
        "edge_params": {"rate": "-10%", "pitch": "-10%", "volume": "-5%"},
        "volc_voice": "zh_male_junlangnanyou_emo_v2_mars_bigtts",
    },
    
    # å…´å¥‹ç±»
    "excited_announce": {
        "desc": "å…´å¥‹/å®£å‘Š/æƒŠå–œ",
        "edge_params": {"rate": "+10%", "pitch": "+15%", "volume": "+10%"},
        "volc_voice": "zh_female_tianmeixiaomei_emo_moon_bigtts",  # ç”œå¿ƒå°å¦¹-å…´å¥‹
    },
    
    # æ„¤æ€’ç±»
    "angry_shout": {
        "desc": "å˜¶å¼/æ„¤æ€’/çˆ†å‘",
        "edge_params": {"rate": "+15%", "pitch": "+10%", "volume": "+20%"},
        "volc_voice": "zh_male_jingqiangkanye_emo_v2_mars_bigtts",  # äº¬è…”ä¾ƒçˆ·-æš´èº
    },
    "fierce_warning": {
        "desc": "çŒ›çƒˆ/è­¦å‘Š/å–‰å“§",
        "edge_params": {"rate": "+10%", "pitch": "+5%", "volume": "+15%"},
        "volc_voice": "zh_male_jingqiangkanye_emo_v2_mars_bigtts",
    },
    
    # å´©æºƒç±»
    "sad_sigh": {
        "desc": "å´©æºƒ/å¹æ¯/å§”å±ˆ",
        "edge_params": {"rate": "-15%", "pitch": "-15%", "volume": "-10%"},
        "volc_voice": "zh_male_junlangnanyou_emo_v2_mars_bigtts",
    },
    
    # å˜²è®½ç±»
    "sarcastic_mock": {
        "desc": "å˜²è®½/å˜²ç¬‘/è½»è”‘",
        "edge_params": {"rate": "+5%", "pitch": "-5%", "volume": "+5%"},
        "volc_voice": "zh_male_jingqiangkanye_emo_v2_mars_bigtts",
    },
    
    # ä¸­æ€§ç±»ï¼ˆé»˜è®¤ï¼‰
    "neutral_narrate": {
        "desc": "ä¸­æ€§/å¹³é™/å™è¿°",
        "edge_params": {"rate": "+0%", "pitch": "+0%", "volume": "+0%"},
        "volc_voice": "zh_male_junlangnanyou_emo_v2_mars_bigtts",
    },
}

# ğŸ”‘ å­—ä½“è·¯å¾„é…ç½®ï¼šå¤šçº§é™çº§ç­–ç•¥ç¡®ä¿100%å¯ç”¨
def get_font_path():
    """æ™ºèƒ½æ£€æµ‹å¯ç”¨çš„ä¸­æ–‡å­—ä½“è·¯å¾„"""
    # 1. ä¼˜å…ˆï¼šassetsç›®å½•ä¸­çš„å­—ä½“æ–‡ä»¶ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    repo_font = os.path.join(os.path.dirname(__file__), "assets", "font.ttf")
    if os.path.exists(repo_font):
        return repo_font
    
    # 2. é™çº§ï¼šå½“å‰å·¥ä½œç›®å½•çš„assets/font.ttf
    if os.path.exists("assets/font.ttf"):
        return os.path.abspath("assets/font.ttf")
    
    # 3. å…¼å®¹æ—§ç‰ˆæœ¬ï¼šæ ¹ç›®å½•çš„font.ttfï¼ˆå‘åå…¼å®¹ï¼‰
    if os.path.exists("font.ttf"):
        return os.path.abspath("font.ttf")
    
    # 3. æœ€ç»ˆé™çº§ï¼šå¯»æ‰¾ç³»ç»Ÿå­—ä½“æ–‡ä»¶
    if platform.system() == "Linux":
        linux_fonts = [
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        ]
        for font in linux_fonts:
            if os.path.exists(font):
                return font
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å› Noneï¼ˆåç»­ä¼šæœ‰é”™è¯¯æç¤ºï¼‰
    return None

FONT_PATH = get_font_path()

# ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šåœ¨ Streamlit ä¾§è¾¹æ æ˜¾ç¤ºå­—ä½“è·¯å¾„
try:
    if st and hasattr(st, 'sidebar'):
        with st.sidebar:
            if FONT_PATH:
                st.success(f"âœ… å­—ä½“åŠ è½½æˆåŠŸ: {os.path.basename(FONT_PATH)}")
            else:
                st.error("âŒ æœªæ‰¾åˆ°å­—ä½“æ–‡ä»¶ï¼è¯·ä¸Šä¼  font.ttf")
except:
    pass  # é Streamlit ç¯å¢ƒä¸‹å¿½ç•¥

# ğŸµ BGM é£æ ¼è·¯ç”±ç³»ç»Ÿ
def get_bgm_by_style(style_name, video_duration):
    """
    æ ¹æ®é£æ ¼éšæœºæŠ½å–ä¸€é¦– BGMï¼Œå¹¶æ ¹æ®è§†é¢‘æ—¶é•¿è‡ªåŠ¨å¾ªç¯å’Œè°ƒæ•´éŸ³é‡
    
    Args:
        style_name: é£æ ¼åç§°ï¼ˆå¦‚ "ğŸ—¡ï¸ è®¤çŸ¥åˆºå®¢æµï¼ˆå†²å‡»åŠ›+ä¼˜è¶Šæ„Ÿï¼‰"ï¼‰
        video_duration: è§†é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
    
    Returns:
        AudioFileClip: å¤„ç†åçš„ BGM éŸ³é¢‘å‰—è¾‘ï¼Œå·²è°ƒæ•´éŸ³é‡å’Œæ—¶é•¿
    """
    # é£æ ¼ä¸æ–‡ä»¶å¤¹çš„æ˜ å°„
    style_folder_map = {
        "ğŸ—¡ï¸ è®¤çŸ¥åˆºå®¢æµï¼ˆå†²å‡»åŠ›+ä¼˜è¶Šæ„Ÿï¼‰": "assassin",
        "ğŸ‘ å¬å‹/å…»æˆç³»ï¼ˆäº’åŠ¨ç‡04+è¯„è®ºçˆ†ç‚¸ï¼‰": "growth",
        "ğŸ¬ POVæ²‰æµ¸æµï¼ˆç¬¬ä¸€äººç§°+ä»£å…¥æ„Ÿï¼‰": "pov",
        "ğŸ”¥ æƒ…ç»ªå®£æ³„æµï¼ˆæè‡´åè½¬+å‘ç–¯æ–‡å­¦ï¼‰": "venting",
        "ğŸ± MemeæŠ—è±¡æµï¼ˆä½æˆæœ¬+ç—…æ¯’ä¼ æ’­ï¼‰": "meme"
    }
    
    folder_name = style_folder_map.get(style_name, "assassin")
    bgm_dir = os.path.join("assets", "bgm", folder_name)
    
    # ä»ç›®å½•ä¸‹éšæœºé€‰ä¸€é¦–æ­Œ
    if os.path.exists(bgm_dir):
        bgm_files = [f for f in os.listdir(bgm_dir) if f.endswith(('.mp3', '.wav'))]
        if bgm_files:
            selected_bgm = random.choice(bgm_files)
            bgm_path = os.path.join(bgm_dir, selected_bgm)
            st.info(f"ğŸµ ä½¿ç”¨ {style_name} é£æ ¼ BGM: {selected_bgm}")
        else:
            # å¦‚æœç›®å½•ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ BGM
            bgm_path = "assets/bgm.mp3"
            st.warning(f"âš ï¸ {folder_name} ç›®å½•ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ BGM")
    else:
        # ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ BGM
        bgm_path = "assets/bgm.mp3"
        st.warning(f"âš ï¸ {bgm_dir} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ BGM")
    
    # æ£€æŸ¥é»˜è®¤ BGM æ˜¯å¦å­˜åœ¨ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    if not os.path.exists(bgm_path):
        # å°è¯•æ—§ç‰ˆæœ¬è·¯å¾„
        if os.path.exists("bgm.mp3"):
            bgm_path = "bgm.mp3"
        else:
            st.error("âŒ æœªæ‰¾åˆ° BGM æ–‡ä»¶ï¼è¯·åœ¨ assets ç›®å½•ä¸‹æ·»åŠ  bgm.mp3")
            return None
    
    try:
        # åŠ è½½éŸ³é¢‘
        bgm_clip = AudioFileClip(bgm_path)
        
        # æ ¸å¿ƒå¤„ç† 1ï¼šå¦‚æœ BGM çŸ­äºè§†é¢‘ï¼Œåˆ™å¾ªç¯æ’­æ”¾
        if bgm_clip.duration < video_duration:
            # ä½¿ç”¨ afx.audio_loop å¾ªç¯æ’­æ”¾
            bgm_clip = afx.audio_loop(bgm_clip, duration=video_duration)
        else:
            # æˆªå–æ‰€éœ€é•¿åº¦
            bgm_clip = bgm_clip.subclip(0, video_duration)
            
        # æ ¸å¿ƒå¤„ç† 2ï¼šè®¾ç½® BGM éŸ³é‡ï¼ˆé€šå¸¸è®¾ä¸º 0.08 - 0.25ï¼Œé¿å…ç›–è¿‡äººå£°ï¼‰
        volume_map = {
            "ğŸ—¡ï¸ è®¤çŸ¥åˆºå®¢æµï¼ˆå†²å‡»åŠ›+ä¼˜è¶Šæ„Ÿï¼‰": 0.15,
            "ğŸ‘ å¬å‹/å…»æˆç³»ï¼ˆäº’åŠ¨ç‡04+è¯„è®ºçˆ†ç‚¸ï¼‰": 0.08,
            "ğŸ¬ POVæ²‰æµ¸æµï¼ˆç¬¬ä¸€äººç§°+ä»£å…¥æ„Ÿï¼‰": 0.12,
            "ğŸ”¥ æƒ…ç»ªå®£æ³„æµï¼ˆæè‡´åè½¬+å‘ç–¯æ–‡å­¦ï¼‰": 0.25,
            "ğŸ± MemeæŠ—è±¡æµï¼ˆä½æˆæœ¬+ç—…æ¯’ä¼ æ’­ï¼‰": 0.20
        }
        
        volume = volume_map.get(style_name, 0.1)
        return bgm_clip.volumex(volume)
        
    except Exception as e:
        st.error(f"âŒ BGM åŠ è½½å¤±è´¥: {e}")
        return None

def create_subtitle_image(text, width=1080, height=400, fontsize=70):
    """ğŸ¨ ç”¨ Pillow æ‰‹å·¥ç»˜åˆ¶å­—å¹•å›¾ç‰‡ï¼ˆå½»åº•ç»•è¿‡ ImageMagickï¼‰"""
    if not FONT_PATH:
        raise FileNotFoundError("æœªæ‰¾åˆ°å­—ä½“æ–‡ä»¶ï¼è¯·ç¡®ä¿ font.ttf å­˜åœ¨äºä»“åº“æ ¹ç›®å½•")
    
    # åˆ›å»ºé€æ˜èƒŒæ™¯å›¾ç‰‡
    img = Image.new('RGBA', (width, height), (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)
    
    # åŠ è½½å­—ä½“
    try:
        font = ImageFont.truetype(FONT_PATH, fontsize)
    except Exception as e:
        st.error(f"å­—ä½“åŠ è½½å¤±è´¥: {e}")
        raise
    
    # æ–‡æœ¬è‡ªåŠ¨æ¢è¡Œ
    lines = []
    words = text
    max_width = width - 100  # å·¦å³è¾¹è·50px
    
    # ç®€å•æ¢è¡Œé€»è¾‘ï¼šæŒ‰å­—ç¬¦å®½åº¦åˆ‡åˆ†
    current_line = ""
    for char in words:
        test_line = current_line + char
        bbox = draw.textbbox((0, 0), test_line, font=font)
        if bbox[2] - bbox[0] > max_width and current_line:
            lines.append(current_line)
            current_line = char
        else:
            current_line = test_line
    if current_line:
        lines.append(current_line)
    
    # è®¡ç®—æ€»é«˜åº¦å¹¶å±…ä¸­
    line_height = fontsize + 20
    total_height = len(lines) * line_height
    start_y = (height - total_height) // 2
    
    # ç»˜åˆ¶æ¯è¡Œæ–‡å­—ï¼ˆå…ˆç”»é»‘è¾¹ï¼Œå†ç”»ç™½å­—ï¼‰
    for i, line in enumerate(lines):
        bbox = draw.textbbox((0, 0), line, font=font)
        text_width = bbox[2] - bbox[0]
        x = (width - text_width) // 2
        y = start_y + i * line_height
        
        # é»‘è‰²æè¾¹ï¼ˆstrokeæ•ˆæœï¼‰
        for offset_x in [-2, 0, 2]:
            for offset_y in [-2, 0, 2]:
                if offset_x != 0 or offset_y != 0:
                    draw.text((x + offset_x, y + offset_y), line, font=font, fill=(0, 0, 0, 255))
        
        # ç™½è‰²ä¸»æ–‡å­—
        draw.text((x, y), line, font=font, fill=(255, 255, 255, 255))
    
    # è½¬ä¸º numpy æ•°ç»„ä¾› MoviePy ä½¿ç”¨
    return np.array(img)

def call_volcengine_tts(text, voice_id, output_path):
    """
    é€šè¿‡è°ƒç”¨å®˜æ–¹ V3 bidirection.py è„šæœ¬æ¥ç”Ÿæˆè±†åŒ…å¤§æ¨¡å‹éŸ³é¢‘
    æ”¯æŒ WebSocket æµå¼ä¼ è¾“ï¼Œé€‚ç”¨äºè±†åŒ…è¯­éŸ³åˆæˆæ¨¡å‹ 2.0
    """
    try:
        # 1. å®‰å…¨è·å–é‰´æƒä¿¡æ¯
        appid = st.secrets.get("VOLC_APPID", "")
        access_token = st.secrets.get("VOLC_ACCESS_TOKEN", "")
        
        if not appid or not access_token:
            # å¦‚æœæ²¡æœ‰é…ç½®ç«å±±å¼•æ“ï¼Œå›é€€åˆ° Edge TTS
            return False
        
        # 2. å®˜æ–¹è„šæœ¬è·¯å¾„
        script_path = os.path.join(os.path.dirname(__file__), "examples", "volcengine", "bidirection.py")
        
        if not os.path.exists(script_path):
            st.error(f"âŒ æ‰¾ä¸åˆ°ç«å±±å¼•æ“ V3 è„šæœ¬: {script_path}")
            return False
        
        st.info(f"ğŸš€ æ­£åœ¨è°ƒç”¨è±†åŒ…è¯­éŸ³åˆæˆå¤§æ¨¡å‹: {voice_id}...")
        
        # 3. æ„å»ºå‘½ä»¤è¡ŒæŒ‡ä»¤
        command = [
            sys.executable,  # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨
            script_path,
            "--appid", appid,
            "--access_token", access_token,
            "--voice_type", voice_id,
            "--text", text,
            "--encoding", "mp3",
            "--output", output_path  # æŒ‡å®šè¾“å‡ºè·¯å¾„
        ]
        
        # 4. æ‰§è¡Œè„šæœ¬
        result = subprocess.run(
            command, 
            check=True, 
            capture_output=True, 
            text=True,
            timeout=60  # 60ç§’è¶…æ—¶
        )
        
        # 5. éªŒè¯è¾“å‡ºæ–‡ä»¶
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            st.success(f"âœ… è±†åŒ…å¤§æ¨¡å‹éŸ³é¢‘æµæ¥æ”¶å®Œæ¯•ï¼éŸ³é¢‘å·²ä¿å­˜è‡³: {output_path}")
            return True
        else:
            st.error(f"âŒ è¾“å‡ºæ–‡ä»¶æœªç”Ÿæˆæˆ–ä¸ºç©º: {output_path}")
            return False
            
    except subprocess.TimeoutExpired:
        st.error("âŒ ç«å±±å¼•æ“ TTS è¶…æ—¶ï¼ˆ60ç§’ï¼‰")
        return False
    except subprocess.CalledProcessError as e:
        st.error(f"âŒ ç«å±±å¤§æ¨¡å‹åˆæˆå¤±è´¥ï¼Œå®˜æ–¹è„šæœ¬æŠ¥é”™ä¿¡æ¯ï¼š")
        st.error(e.stderr)
        return False
    except Exception as e:
        st.error(f"âŒ ç«å±±å¼•æ“ TTS è°ƒç”¨å¼‚å¸¸: {e}")
        return False

async def text_to_mp3(text, filename, voice_id="zh-CN-YunxiNeural"):
    """ã€äº‘ç«¯ä¼˜åŒ–ç‰ˆã€‘ç›´æ¥è”ç½‘ç”Ÿæˆé…éŸ³ï¼Œå¢åŠ é‡è¯•é€»è¾‘ã€‚æ”¯æŒå¤šè·¯ TTS è·¯ç”±ã€‚"""
    
    # ğŸ¹ï¸ è·¯ç”± 1ï¼šç«å±±å¼•æ“ TTS (æ–¹è¨€ + é«˜æƒ…ç»ªè¡¨è¾¾)
    if voice_id.startswith("volc_"):
        # å»æ‰å‰ç¼€ï¼Œè·å–çœŸå®çš„éŸ³è‰² ID
        real_voice_id = voice_id.replace("volc_", "")
        success = call_volcengine_tts(text, real_voice_id, filename)
        if success:
            return True
        else:
            # ç«å±±å¼•æ“å¤±è´¥ï¼Œå›é€€åˆ° Edge TTS
            st.warning("âš ï¸ ç«å±±å¼•æ“ä¸å¯ç”¨ï¼Œå›é€€åˆ° Edge TTS æ¨¡å¼")
            voice_id = "zh-CN-YunxiNeural"  # ä½¿ç”¨é»˜è®¤ç”·å£°
    
    # ğŸ¹ï¸ è·¯ç”± 2ï¼šEdge TTS (å…è´¹å…¼åº•)
    for attempt in range(3):
        try:
            # ğŸµ æ”¯æŒSSMLæƒ…ç»ªæ ‡ç­¾ï¼šå¦‚æœæ–‡æœ¬ä¸­åŒ…å«<prosody>æ ‡ç­¾ï¼ŒEdge TTSä¼šè‡ªåŠ¨è¯†åˆ«
            # æ³¨æ„ï¼šEdge TTSåŸç”Ÿæ”¯æŒSSMLï¼Œç›´æ¥ä¼ å…¥åŒ…å«<prosody>çš„æ–‡æœ¬å³å¯
            communicate = edge_tts.Communicate(text, voice_id, rate="+10%")
            await communicate.save(filename)
            
            # ğŸ”¥ æ–°å¢ï¼šéªŒè¯æ–‡ä»¶æ˜¯å¦ç”ŸæˆæˆåŠŸ
            if os.path.exists(filename) and os.path.getsize(filename) > 0:
                return True
            else:
                st.error(f"âŒ éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©º: {filename}")
                return False
                
        except Exception as e:
            st.warning(f"TTS å°è¯• {attempt+1}/3 å¤±è´¥: {e}")
            await asyncio.sleep(2)
    
    st.error(f"âŒ éŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼ˆ3æ¬¡é‡è¯•åï¼‰: {filename}")
    return False

# ğŸ¬ ç‰‡æ®µå¼æƒ…ç»ªå¼•æ“ (Segmented Emotional Engine)
async def synthesize_emotional_segment(text, vibe, output_file, use_volcengine=False):
    """
    æ ¹æ®æƒ…ç»ªæ ‡ç­¾åˆæˆå•ä¸ªéŸ³é¢‘ç‰‡æ®µ
    
    Args:
        text: æ–‡æ¡ˆå†…å®¹
        vibe: æƒ…ç»ªæ ‡ç­¾ï¼ˆå¦‚ "cold_question", "angry_shout"ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        use_volcengine: æ˜¯å¦ä½¿ç”¨ç«å±±å¼•æ“ï¼ˆé»˜è®¤Falseä½¿ç”¨Edge TTSï¼‰
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    # è·å–æƒ…ç»ªå‚æ•°ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ä½¿ç”¨é»˜è®¤
    vibe_config = VIBE_ROUTING_TABLE.get(vibe, VIBE_ROUTING_TABLE["neutral_narrate"])
    
    if use_volcengine:
        # ä½¿ç”¨ç«å±±å¼•æ“ï¼šç›´æ¥è°ƒç”¨ï¼Œé€šè¿‡éŸ³è‰²åˆ‡æ¢å®ç°æƒ…ç»ª
        voice_id = vibe_config["volc_voice"]
        success = call_volcengine_tts(text, voice_id, output_file)
        return success
    else:
        # ä½¿ç”¨ Edge TTSï¼šé€šè¿‡å‚æ•°æ§åˆ¶
        params = vibe_config["edge_params"]
        try:
            communicate = edge_tts.Communicate(
                text, 
                "zh-CN-YunxiNeural",  # åŸºç¡€éŸ³è‰²
                rate=params["rate"],
                pitch=params["pitch"],
                volume=params["volume"]
            )
            await communicate.save(output_file)
            
            if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                return True
            else:
                return False
        except Exception as e:
            st.warning(f"âš ï¸ æƒ…ç»ªç‰‡æ®µ [{vibe}] ç”Ÿæˆå¤±è´¥: {e}")
            return False

async def synthesize_emotional_segments_parallel(segments, use_volcengine=False):
    """
    å¹¶è¡Œåˆæˆå¤šä¸ªæƒ…ç»ªç‰‡æ®µï¼ˆæ ¸å¿ƒåŠ é€Ÿé€»è¾‘ï¼‰
    
    Args:
        segments: ç‰‡æ®µåˆ—è¡¨ [{"text": "...", "vibe": "..."}, ...]
        use_volcengine: æ˜¯å¦ä½¿ç”¨ç«å±±å¼•æ“
    
    Returns:
        list: æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    tasks = []
    output_files = []
    
    for i, seg in enumerate(segments):
        output_file = f"temp_emotional_segment_{i}_{uuid.uuid4().hex[:8]}.mp3"
        output_files.append(output_file)
        
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        task = synthesize_emotional_segment(
            text=seg.get("text", ""),
            vibe=seg.get("vibe", "neutral_narrate"),
            output_file=output_file,
            use_volcengine=use_volcengine
        )
        tasks.append(task)
    
    # ğŸš€ å…³é”®ï¼šå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼ˆ5ä¸ªç‰‡æ®µ = 1ä¸ªç‰‡æ®µçš„æ—¶é—´ï¼‰
    st.info(f"ğŸ¬ å¹¶è¡Œåˆæˆ {len(segments)} ä¸ªæƒ…ç»ªç‰‡æ®µ...")
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # éªŒè¯ç»“æœ
    success_files = []
    for i, (result, file) in enumerate(zip(results, output_files)):
        if result is True and os.path.exists(file):
            success_files.append(file)
            st.success(f"âœ… ç‰‡æ®¶ {i+1}/{len(segments)}: {segments[i].get('vibe', 'neutral')} - æˆåŠŸ")
        else:
            st.error(f"âŒ ç‰‡æ®¶ {i+1}/{len(segments)}: å¤±è´¥")
            success_files.append(None)
    
    return success_files

def concatenate_audio_segments_with_breath(audio_files, output_path, breath_duration=0.2):
    """
    æ‹¼æ¥éŸ³é¢‘ç‰‡æ®µï¼Œå¹¶åœ¨æ¥ç¼å¤„æ’å…¥å‘¼å¸åœé¡¿ï¼ˆå¢å¼ºçœŸäººæ„Ÿï¼‰
    
    Args:
        audio_files: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        breath_duration: å‘¼å¸åœé¡¿æ—¶é•¿ï¼ˆç§’ï¼‰
    
    Returns:
        str: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        # åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„éŸ³é¢‘ç‰‡æ®¶
        clips = []
        for i, file in enumerate(audio_files):
            if file and os.path.exists(file):
                try:
                    clip = AudioFileClip(file)
                    clips.append(clip)
                    
                    # åœ¨ç‰‡æ®¶ä¹‹é—´æ’å…¥é™éŸ³ï¼ˆæ¨¡æ‹Ÿå‘¼å¸ï¼‰
                    if i < len(audio_files) - 1:  # ä¸åœ¨æœ€åä¸€ä¸ªåé¢åŠ 
                        # åˆ›å»ºé™éŸ³ç‰‡æ®¶
                        silence = AudioFileClip(file).volumex(0).subclip(0, breath_duration)
                        clips.append(silence)
                except Exception as e:
                    st.warning(f"âš ï¸ ç‰‡æ®¶ {i+1} åŠ è½½å¤±è´¥: {e}")
        
        if not clips:
            st.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„éŸ³é¢‘ç‰‡æ®¶")
            return None
        
        # ğŸµ æ‹¼æ¥æ‰€æœ‰ç‰‡æ®¶
        final_audio = concatenate_audioclips(clips)
        final_audio.write_audiofile(output_path, codec='libmp3lame')
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for clip in clips:
            clip.close()
        for file in audio_files:
            if file and os.path.exists(file):
                try:
                    os.remove(file)
                except:
                    pass
        
        st.success(f"âœ… éŸ³é¢‘æ‹¼æ¥å®Œæˆï¼Œå…± {len(clips)} ä¸ªç‰‡æ®¶")
        return output_path
        
    except Exception as e:
        st.error(f"âŒ éŸ³é¢‘æ‹¼æ¥å¤±è´¥: {e}")
        return None

def generate_all_audios_sync(scenes_data, voice_id="zh-CN-YunxiNeural"):
    """ä¸²è¡Œç”Ÿæˆæ‰€æœ‰åˆ†é•œé…éŸ³"""
    audio_files = []
    failed_count = 0
    
    for i, scene in enumerate(scenes_data):
        audio_file = f"temp_audio_{i}.mp3"
        st.toast(f"ğŸ¹ï¸ AI é…éŸ³ç”Ÿæˆä¸­... {i+1}/{len(scenes_data)}")
        
        # ğŸ”¥ æ–°å¢ï¼šæ˜¾ç¤ºå½“å‰å¤„ç†çš„æ–‡æœ¬ï¼ˆå‰50ä¸ªå­—ç¬¦ï¼‰
        narration_preview = scene['narration'][:50] + "..." if len(scene['narration']) > 50 else scene['narration']
        st.caption(f"ğŸ“ æ­£åœ¨å¤„ç†: {narration_preview}")
        
        try:
            success = asyncio.run(text_to_mp3(scene['narration'], audio_file, voice_id))
            if success:
                audio_files.append(audio_file)
                st.success(f"âœ… åˆ†é•œ {i+1} éŸ³é¢‘ç”ŸæˆæˆåŠŸ")
            else:
                audio_files.append(None)
                failed_count += 1
                st.error(f"âŒ åˆ†é•œ {i+1} éŸ³é¢‘ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            audio_files.append(None)
            failed_count += 1
            st.error(f"âŒ åˆ†é•œ {i+1} éŸ³é¢‘ç”Ÿæˆå¼‚å¸¸: {e}")
        
        asyncio.run(asyncio.sleep(0.5))
    
    # ğŸ”¥ æ–°å¢ï¼šæ˜¾ç¤ºæ€»ç»“
    if failed_count > 0:
        st.warning(f"âš ï¸ éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼Œä½†æœ‰ {failed_count}/{len(scenes_data)} ä¸ªå¤±è´¥")
    else:
        st.success(f"âœ… æ‰€æœ‰ {len(scenes_data)} ä¸ªéŸ³é¢‘ç”ŸæˆæˆåŠŸï¼")
    
    return audio_files

def render_ai_video_pipeline(scenes_data, zhipu_key, output_path, pexels_key=None, voice_id="zh-CN-YunxiNeural", style_name=None):
    """æ ¸å¿ƒè§†é¢‘æ¸²æŸ“ç®¡çº¿
    
    Args:
        scenes_data: åˆ†é•œæ•°æ®åˆ—è¡¨
        zhipu_key: æ™ºè°± API Key
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        pexels_key: Pexels API Key
        voice_id: å£°éŸ³ ID
        style_name: é£æ ¼åç§°ï¼ˆç”¨äºåŒ¹é… BGMï¼‰
    """
    from api_services import generate_images_zhipu
    
    # 1. èµ„æºç”Ÿæˆ
    image_paths = generate_images_zhipu(scenes_data, zhipu_key)
    audio_files = generate_all_audios_sync(scenes_data, voice_id)  # ä¼ é€’ voice_id
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæˆåŠŸç”Ÿæˆçš„å›¾ç‰‡æ•°é‡
    success_count = sum(1 for p in image_paths if p)
    st.write(f"ğŸ“¸ æˆåŠŸç”Ÿæˆå›¾ç‰‡æ•°é‡: {success_count}/{len(image_paths)}")
    
    # ğŸ” æ–°å¢ï¼šè°ƒè¯•éŸ³é¢‘æ–‡ä»¶çŠ¶æ€
    audio_success_count = sum(1 for a in audio_files if a and os.path.exists(a))
    st.write(f"ğŸ¹ï¸ æˆåŠŸç”ŸæˆéŸ³é¢‘æ•°é‡: {audio_success_count}/{len(audio_files)}")
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœæ‰€æœ‰éŸ³é¢‘éƒ½å¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯
    if audio_success_count == 0:
        st.error("âŒ æ‰€æœ‰éŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–TTSé…ç½®")
        return False
    
    scene_clips = []
    temp_files = []

    # 2. é€åˆ†é•œåˆæˆ
    for i, scene in enumerate(scenes_data):
        # ğŸ”¥ ä¿®å¤ï¼šå…ˆæ£€æŸ¥audio_files[i]æ˜¯å¦ä¸ºNoneï¼Œå†æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not audio_files[i] or not os.path.exists(audio_files[i]): 
            st.warning(f"âš ï¸ åˆ†é•œ {i+1} éŸ³é¢‘ç”Ÿæˆå¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
            
        try:
            audio_clip = AudioFileClip(audio_files[i])
            dur = audio_clip.duration
            temp_files.append(audio_files[i])
        except Exception as e:
            st.error(f"âŒ åˆ†é•œ {i+1} éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
            continue
        
        # ç”»é¢é€»è¾‘ï¼šAIç»˜ç”» > é»‘å±å ä½
        if image_paths[i]:
            st.write(f"ğŸ–¼ï¸ åˆ†é•œ {i+1} ä½¿ç”¨AIç»˜ç”»: {image_paths[i]}")
            try:
                # ğŸ”‘ æ ¸å¿ƒä¿®å¤ï¼šç”¨ Pillow é¢„å¤„ç†å›¾ç‰‡ï¼Œé¿å… MoviePy çš„ resize è§¦å‘ ANTIALIAS
                from PIL import Image as PILImage
                img = PILImage.open(image_paths[i])
                
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆç›®æ ‡é«˜åº¦ 1920ï¼‰
                scale = 1920 / img.height
                new_width = int(img.width * scale)
                
                # ä½¿ç”¨ Pillow çš„ LANCZOS é‡é‡‡æ ·ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
                try:
                    # Pillow >= 10.0.0
                    img_resized = img.resize((new_width, 1920), PILImage.Resampling.LANCZOS)
                except AttributeError:
                    # Pillow < 10.0.0
                    img_resized = img.resize((new_width, 1920), PILImage.LANCZOS)
                
                # è£å‰ªåˆ° 1080x1920ï¼ˆå±…ä¸­è£å‰ªï¼‰
                left = (new_width - 1080) // 2
                img_cropped = img_resized.crop((left, 0, left + 1080, 1920))
                
                # è½¬ä¸º numpy æ•°ç»„ï¼Œä¼ ç»™ MoviePyï¼ˆä¸å†è°ƒç”¨ resizeï¼‰
                img_array = np.array(img_cropped)
                bg = ImageClip(img_array).set_duration(dur)
                temp_files.append(image_paths[i])
                st.success(f"âœ… åˆ†é•œ {i+1} å›¾ç‰‡å¤„ç†æˆåŠŸ")
            except Exception as e:
                st.error(f"âŒ åˆ†é•œ {i+1} å›¾ç‰‡åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»‘å±å ä½")
                bg = ColorClip(size=(1080, 1920), color=(0, 0, 0)).set_duration(dur)
        else:
            st.write(f"âš« åˆ†é•œ {i+1} å›¾ç‰‡ä¸ºç©ºï¼Œä½¿ç”¨é»‘å±å ä½")
            # ğŸ”‘ ä¿®å¤ï¼šä½¿ç”¨ ColorClip åˆ›å»ºçº¯é»‘èƒŒæ™¯
            bg = ColorClip(size=(1080, 1920), color=(0, 0, 0)).set_duration(dur)

        # ğŸ¨ å­—å¹•é€»è¾‘ï¼šç”¨ Pillow æ‰‹å·¥ç»˜åˆ¶ + æ­£ç¡®å¤„ç†é€æ˜åº¦
        subtitle_rgba = create_subtitle_image(scene['narration'], width=1080, height=400, fontsize=70)
        
        # ğŸ”‘ æ ¸å¿ƒä¿®å¤ï¼šæ‹†åˆ† RGB å’Œ Alpha é€šé“ï¼Œç¡®ä¿é€æ˜åº¦æ­£ç¡®
        # RGBA æ•°ç»„çš„å‰3ä¸ªé€šé“æ˜¯é¢œè‰²ï¼Œç¬¬4ä¸ªé€šé“æ˜¯é€æ˜åº¦
        rgb_array = subtitle_rgba[:, :, :3]  # å–å‰3ä¸ªé€šé“ï¼ˆRGBï¼‰
        alpha_array = subtitle_rgba[:, :, 3] / 255.0  # å–ç¬¬4ä¸ªé€šé“ï¼ˆAlphaï¼‰ï¼Œå½’ä¸€åŒ–åˆ°0-1
        
        # åˆ›å»ºå­—å¹•å›¾å±‚ï¼Œæ˜ç¡®æŒ‡å®š mask
        txt_clip = ImageClip(rgb_array).set_duration(dur)
        txt_clip = txt_clip.set_mask(ImageClip(alpha_array, ismask=True).set_duration(dur))
        txt_clip = txt_clip.set_position(('center', 0.75), relative=True)
        
        scene_clips.append(CompositeVideoClip([bg, txt_clip]).set_audio(audio_clip))

    # 3. æœ€ç»ˆå‹åˆ¶ä¸ BGM æ··éŸ³
    if not scene_clips: return False
    
    final = concatenate_videoclips(scene_clips, method="compose")
    
    # ğŸµ ä½¿ç”¨æ–°çš„ BGM é£æ ¼è·¯ç”±ç³»ç»Ÿ
    if style_name:
        st.write(f"ğŸµ æ ¹æ® {style_name} é£æ ¼åŒ¹é… BGM...")
        bgm_clip = get_bgm_by_style(style_name, final.duration)
        if bgm_clip:
            # æ··åˆäººå£°å’Œ BGM
            final = final.set_audio(CompositeAudioClip([
                final.audio.volumex(1.2),  # ç¨å¾®è°ƒé«˜äººå£°ï¼Œç¡®ä¿æ¸…æ™°
                bgm_clip
            ]))
        else:
            st.warning("âš ï¸ BGM åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé£æ ¼ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤ BGMï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
        default_bgm_paths = ["assets/bgm.mp3", "bgm.mp3"]
        bgm_path = None
        for path in default_bgm_paths:
            if os.path.exists(path):
                bgm_path = path
                break
        
        if bgm_path:
            st.info("ğŸµ ä½¿ç”¨é»˜è®¤ BGM")
            bgm = AudioFileClip(bgm_path).volumex(0.08).set_duration(final.duration)
            final = final.set_audio(CompositeAudioClip([final.audio, bgm]))

    # 4. å¯¼å‡º (ä¼˜åŒ–å‚æ•°é˜²æ­¢äº‘ç«¯å†…å­˜æº¢å‡º)
    final.write_videofile(output_path, fps=24, codec="libx264", audio_codec="aac", 
                          threads=4, preset="ultrafast", logger=None)
    
    # 5. èµ„æºæ¸…ç†
    final.close()
    for f in temp_files:
        if f and os.path.exists(f): 
            try: os.remove(f)
            except: pass
    return True

# ğŸ¬ å¯¼æ¼”æ—¶é—´è½´å¼•æ“ (Director's Timeline Engine)
class VideoAssembler:
    """
    åŸºäº Manifest JSON çš„ä¸€é”®æ··å‰ªå¼•æ“
    è§£å†³éŸ³ç”»åŒæ­¥ã€SFXè‡ªåŠ¨åŒ¹é…ã€æƒ…ç»ªè¯­éŸ³ç­‰æ ¸å¿ƒé—®é¢˜
    """
    
    # ğŸ”Š éŸ³æ•ˆåº“è·¯ç”±è¡¨
    SFX_LIBRARY = {
        "heartbeat_heavy": "assets/sfx/heartbeat_heavy.mp3",
        "glass_shatter": "assets/sfx/glass_shatter.mp3",
        "whoosh": "assets/sfx/whoosh.mp3",
        "tension_riser": "assets/sfx/tension_riser.mp3",
        "emotional_swell": "assets/sfx/emotional_swell.mp3",
        "silence": None  # é™éŸ³ï¼Œä¸åŠ è½½éŸ³æ•ˆ
    }
    
    def __init__(self, manifest_data, voice_id="zh-CN-YunxiNeural", use_volcengine=False):
        """
        Args:
            manifest_data: å¯¼æ¼”æ—¶é—´è½´ JSON åˆ—è¡¨
            voice_id: TTS éŸ³è‰² ID
            use_volcengine: æ˜¯å¦ä½¿ç”¨ç«å±±å¼•æ“
        """
        self.manifest = manifest_data
        self.voice_id = voice_id
        self.use_volcengine = use_volcengine
        self.validate_manifest()
    
    def validate_manifest(self):
        """éªŒè¯ Manifest æ ¼å¼åˆæ³•æ€§"""
        required_fields = ["start_time", "end_time", "narration", "emotion_vibe", "image_prompt"]
        
        for i, segment in enumerate(self.manifest):
            for field in required_fields:
                if field not in segment:
                    raise ValueError(f"åˆ†é•œ {i+1} ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
            
            # éªŒè¯æ—¶é—´è¿ç»­æ€§
            if i > 0:
                prev_end = self.manifest[i-1]["end_time"]
                curr_start = segment["start_time"]
                if curr_start != prev_end:
                    st.warning(f"âš ï¸ æ—¶é—´è½´ä¸è¿ç»­ï¼šåˆ†é•œ{i} ç»“æŸäº {prev_end}sï¼Œä½†åˆ†é•œ{i+1} å¼€å§‹äº {curr_start}s")
        
        st.success(f"âœ… Manifest éªŒè¯é€šè¿‡ï¼š{len(self.manifest)} ä¸ªåˆ†é•œï¼Œæ€»æ—¶é•¿ {self.manifest[-1]['end_time']}s")
    
    async def synthesize_segment_with_emotion(self, segment, output_file):
        """
        æ ¹æ® emotion_vibe åˆæˆå•ä¸ªéŸ³é¢‘ç‰‡æ®µ
        """
        text = segment["narration"]
        vibe = segment.get("emotion_vibe", "neutral_narrate")
        
        # è°ƒç”¨ç‰‡æ®µå¼æƒ…ç»ªå¼•æ“
        success = await synthesize_emotional_segment(
            text=text,
            vibe=vibe,
            output_file=output_file,
            use_volcengine=self.use_volcengine
        )
        
        return success
    
    async def synthesize_all_audio_parallel(self):
        """
        å¹¶è¡Œåˆæˆæ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        Returns: [(audio_file, sfx_file, start, end), ...]
        """
        tasks = []
        audio_info = []
        
        for i, segment in enumerate(self.manifest):
            audio_file = f"temp_timeline_audio_{i}_{uuid.uuid4().hex[:8]}.mp3"
            audio_info.append({
                "audio_file": audio_file,
                "sfx": segment.get("sfx"),
                "start": segment["start_time"],
                "end": segment["end_time"]
            })
            
            task = self.synthesize_segment_with_emotion(segment, audio_file)
            tasks.append(task)
        
        # ğŸš€ å¹¶è¡Œæ‰§è¡Œ
        st.info(f"ğŸ¬ å¹¶è¡Œåˆæˆ {len(tasks)} ä¸ªæƒ…ç»ªéŸ³é¢‘ç‰‡æ®µ...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # éªŒè¯ç»“æœ
        success_info = []
        for i, (result, info) in enumerate(zip(results, audio_info)):
            if result is True and os.path.exists(info["audio_file"]):
                success_info.append(info)
                emotion = self.manifest[i].get("emotion_vibe", "neutral")
                st.success(f"âœ… åˆ†é•œ {i+1}: {emotion} - éŸ³é¢‘åˆæˆæˆåŠŸ")
            else:
                st.error(f"âŒ åˆ†é•œ {i+1}: éŸ³é¢‘åˆæˆå¤±è´¥")
        
        return success_info
    
    def load_sfx(self, sfx_name):
        """
        åŠ è½½éŸ³æ•ˆæ–‡ä»¶
        Returns: AudioFileClip æˆ– None
        """
        if not sfx_name or sfx_name == "silence":
            return None
        
        sfx_path = self.SFX_LIBRARY.get(sfx_name)
        if sfx_path and os.path.exists(sfx_path):
            try:
                return AudioFileClip(sfx_path)
            except Exception as e:
                st.warning(f"âš ï¸ éŸ³æ•ˆ {sfx_name} åŠ è½½å¤±è´¥: {e}")
        else:
            st.warning(f"âš ï¸ éŸ³æ•ˆ {sfx_name} ä¸å­˜åœ¨")
        
        return None
    
    def assemble_timeline_audio(self, audio_info_list, output_path):
        """
        æŒ‰ç…§æ—¶é—´è½´ç»„è£…éŸ³é¢‘ï¼ˆåŒ…æ‹¬ TTS + SFXï¼‰
        """
        try:
            audio_clips = []
            
            for info in audio_info_list:
                # åŠ è½½ TTS éŸ³é¢‘
                if os.path.exists(info["audio_file"]):
                    tts_clip = AudioFileClip(info["audio_file"])
                    
                    # åŠ è½½ SFX
                    sfx_clip = self.load_sfx(info["sfx"])
                    
                    if sfx_clip:
                        # æ··åˆ TTS + SFX
                        combined = CompositeAudioClip([tts_clip, sfx_clip.volumex(0.3)])
                        audio_clips.append(combined)
                    else:
                        audio_clips.append(tts_clip)
            
            if not audio_clips:
                st.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„éŸ³é¢‘ç‰‡æ®µ")
                return None
            
            # ğŸµ æ‹¼æ¥æ‰€æœ‰ç‰‡æ®¶
            final_audio = concatenate_audioclips(audio_clips)
            final_audio.write_audiofile(output_path, codec='libmp3lame')
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for clip in audio_clips:
                clip.close()
            for info in audio_info_list:
                if os.path.exists(info["audio_file"]):
                    try:
                        os.remove(info["audio_file"])
                    except:
                        pass
            
            st.success(f"âœ… æ—¶é—´è½´éŸ³é¢‘ç»„è£…å®Œæˆï¼Œå…± {len(audio_clips)} ä¸ªç‰‡æ®¶")
            return output_path
            
        except Exception as e:
            st.error(f"âŒ éŸ³é¢‘ç»„è£…å¤±è´¥: {e}")
            return None
    
    async def render_video_from_manifest(self, output_path="final_video.mp4", bgm_style=None):
        """
        ğŸ¬ ä¸€é”®æ··å‰ªï¼šä» Manifest ç”Ÿæˆå®Œæ•´è§†é¢‘
        """
        st.info("ğŸ¬ å¼€å§‹åŸºäºå¯¼æ¼”æ—¶é—´è½´çš„è§†é¢‘æ¸²æŸ“...")
        
        # 1. å¹¶è¡Œåˆæˆæ‰€æœ‰éŸ³é¢‘
        audio_info_list = await self.synthesize_all_audio_parallel()
        
        if not audio_info_list:
            st.error("âŒ éŸ³é¢‘åˆæˆå¤±è´¥")
            return False
        
        # 2. ç»„è£…æ—¶é—´è½´éŸ³é¢‘ï¼ˆTTS + SFXï¼‰
        timeline_audio = self.assemble_timeline_audio(audio_info_list, "temp_timeline_audio.mp3")
        
        if not timeline_audio:
            return False
        
        # 3. TODO: ç”Ÿæˆå›¾ç‰‡å¹¶ç»„è£…è§†é¢‘ï¼ˆå¤ç”¨ç°æœ‰ render_ai_video_pipeline é€»è¾‘ï¼‰
        # è¿™é‡Œæš‚æ—¶è¿”å›æˆåŠŸï¼Œå®Œæ•´å®ç°éœ€è¦æ•´åˆå›¾ç‰‡ç”Ÿæˆå’Œ MoviePy æ¸²æŸ“
        st.success("âœ… æ—¶é—´è½´éŸ³é¢‘ç”ŸæˆæˆåŠŸï¼")
        st.info("ğŸš§ è§†é¢‘æ¸²æŸ“åŠŸèƒ½å¾…å®Œå–„ï¼Œå½“å‰ä»…ç”ŸæˆéŸ³é¢‘è½¨")
        
        return True
